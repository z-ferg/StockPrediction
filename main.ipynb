{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8c423",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "177b96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Local Dataset Importing\n",
    "        Originally tried to use the Kaggle API however quickly faced\n",
    "        issues with rate limiting. Local downloading will speed up\n",
    "        training at the cost of repeatability for others viewing project.\n",
    "        \n",
    "        Solution, I pull from downloaded datasets in parent folder to this\n",
    "        github repository. news_data contains only the news dataset .json file,\n",
    "        stock data contains a .csv with the metadata as well as 2 subfolders\n",
    "        ETF and Stocks that contain their respective ticker histories\n",
    "\"\"\"\n",
    "news_path = os.getcwd() + \"/../news_data/\"\n",
    "stock_path = os.getcwd() + \"/../stock_data/\"\n",
    "\n",
    "news_df = pd.read_json(f'{news_path}News_Category_Dataset_v3.json', lines=True)\n",
    "stock_meta = pd.read_csv(f'{stock_path}symbols_valid_meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8e0d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with stock: AGM$A\n",
      "Error with stock: CARR.V\n",
      "Error with stock: UTX.V\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Capture all Data Frames\n",
    "        Maintain a dictionary with keys as stock ticker symbols and values\n",
    "        as the DataFrames captured from reading the stock's .csv\n",
    "\"\"\"\n",
    "stock_tickers = [x['Symbol'] for _, x in stock_meta.iterrows() if x['ETF'] == 'N'] # Remove ETFs\n",
    "\n",
    "\n",
    "stock_dfs = {}\n",
    "for s in stock_tickers:\n",
    "    try:\n",
    "        stock_dfs[s] = pd.read_csv(f\"{stock_path}/stocks/{s}.csv\")\n",
    "    except Exception:\n",
    "        print(f\"Error with stock: {s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7606494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Columns: ['link' 'headline' 'category' 'short_description' 'authors' 'date']\n",
      "Stock Columns: ['Date' 'Open' 'High' 'Low' 'Close' 'Adj Close' 'Volume']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset Columns\n",
    "\"\"\"\n",
    "print(f\"News Columns: {news_df.columns.values}\")\n",
    "print(f\"Stock Columns: {stock_dfs['A'].columns.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f1058e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "# Return the next trading day, avoiding holidays and weekends\n",
    "#   Inputs\n",
    "#       cur_day      -> The current date\n",
    "#       trading_days -> List of all open dates in the range\n",
    "#   Returns:\n",
    "#       The next available date (datetime object)\n",
    "def next_trading_day(cur_day, trading_days):\n",
    "    days_left = trading_days[trading_days > cur_day]\n",
    "    return days_left.min() if len(days_left) else trading_days.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5209ccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stocks before processing: \t5881\n",
      "Number of Stocks after processing: \t3511\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Prepare Stock Datasets\n",
    "        Drop irrelevant columns --> High, Low, Volume, adj_close\n",
    "        Drop information prior to 2012-01-28\n",
    "        Create stock return time horizon features\n",
    "\"\"\"\n",
    "processed_stocks = {}\n",
    "tickers = stock_dfs.keys()\n",
    "\n",
    "for s in tickers:\n",
    "    df = stock_dfs[s].copy()\n",
    "\n",
    "    df['Ticker'] = s\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df[df['Date'] >= pd.Timestamp(\"2012-01-28\")].reset_index(drop=True)\n",
    "    df = df.drop(['High', 'Low', 'Adj Close', 'Volume'], axis=1)\n",
    "\n",
    "    if df.shape[0] != 2057: # 2057 dates 2012-01-28 and 2020-04-01, cut unfilled stocks\n",
    "        continue\n",
    "\n",
    "    df['r_0d'] = (df['Close'] - df['Open'])/df['Open']\n",
    "    df['r_1d'] = df['Close'].shift(-1).pct_change(periods=1, fill_method=None)\n",
    "    df['r_7d'] = df['Close'].shift(-7).pct_change(periods=7, fill_method=None)\n",
    "    df['r_30d'] = df['Close'].shift(-30).pct_change(periods=30, fill_method=None)\n",
    "\n",
    "    processed_stocks[s] = df\n",
    "\n",
    "print(f\"Number of Stocks before processing: \\t{len(stock_dfs)}\")\n",
    "print(f\"Number of Stocks after processing: \\t{len(processed_stocks)}\")\n",
    "\n",
    "stock_dfs = {k: v.dropna().reset_index(drop=True) for k, v in processed_stocks.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefdb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Prepare News Dataset\n",
    "        News DF starts 2022-09-23, ends 2012-01-28 --> First must reverse dataset\n",
    "        Drop categories to only necessary --> Category, Headline, Date\n",
    "        Shift dates to align with trading days, skipping weekends and holidays till next open day\n",
    "\"\"\"\n",
    "news_df = news_df.sort_values(by='date', ascending=True).reset_index(drop=True)\n",
    "news_df = news_df.drop(['link', 'short_description', 'authors'], axis=1)\n",
    "news_df = news_df[news_df['date'] <= pd.Timestamp(\"2020-04-01\")].reset_index(drop=True)\n",
    "\n",
    "trading_days = pd.to_datetime(stock_dfs['A']['Date'].unique())\n",
    "news_df['effective_date'] = news_df['date'].apply(lambda d: next_trading_day(d, trading_days))\n",
    "\n",
    "# Stock ticker named TECH has issues down the line\n",
    "news_df['category'] = news_df['category'].replace({'TECH': 'TECHNOLOGY'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa29191",
   "metadata": {},
   "source": [
    "# Step 2: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "48bee106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment of each headline using \n",
    "news_df['sentiment'] = news_df['headline'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "48e083c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment = (\n",
    "    news_df.groupby(['effective_date', 'category'])\n",
    "    .agg(\n",
    "        avg_sentiment = ('sentiment', 'mean'),\n",
    "        article_count = ('headline', 'count')\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "34bfc8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = pd.date_range(news_df['effective_date'].min(), news_df['effective_date'].max())\n",
    "all_cats = news_df['category'].unique()\n",
    "\n",
    "\n",
    "all_cats_per_date = pd.MultiIndex.from_product(\n",
    "    [all_dates, all_cats],\n",
    "    names=['date', 'category']\n",
    ")\n",
    "\n",
    "daily_sentiment = (\n",
    "    daily_sentiment\n",
    "    .set_index(['effective_date', 'category'])\n",
    "    .reindex(all_cats_per_date)\n",
    "    .fillna({'avg_sentiment': 0, 'article_count': 0})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c9824",
   "metadata": {},
   "source": [
    "# 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b88ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    At this point:\n",
    "        - stock_dfs: dictionary with ticker as key, DF as value\n",
    "            - Gather list of tickers being used through stock_dfs.keys()\n",
    "            - stock_dfs[...]\n",
    "                - Date, Open, Close, Ticker, r_0d, r_1d, r_7d, r_30d\n",
    "        - daily_sentiment: DF holding sentiment scores\n",
    "            - date, category (42 total), avg_sentiment, article_count\n",
    "    \n",
    "    The plan:\n",
    "        - For each time horizon create an XGBoost Model\n",
    "            - Each model gets all 3511 stock openings and the scores of that day\n",
    "                - Target is the designated time horizon of that modelSo \n",
    "\"\"\"\n",
    "all_rows = []\n",
    "all_dates = stock_dfs['A']['Date']\n",
    "\n",
    "cols = ['Date', 'Ticker', 'Open', 'r_0d', 'r_1d', 'r_7d', 'r_30d']\n",
    "cols.extend(all_cats)\n",
    "\n",
    "for d in all_dates:\n",
    "    # Get dictionary of daily sentiments\n",
    "    sent_df = daily_sentiment[daily_sentiment['date'] == d]\n",
    "    daily_sent_scores = [sent_df[sent_df['category'] == cat]['avg_sentiment'].item() for cat in all_cats]\n",
    "    \n",
    "    # Iterate over tickers\n",
    "    for ticker, df in stock_dfs.items():\n",
    "        date, open, close, tick, r0, r1, r7, r30 = df[df['Date'] == d].values[0]\n",
    "        \n",
    "        row = [d, tick, open, r0, r1, r7, r30] # Get the date and stock ticker\n",
    "        row.extend(daily_sent_scores)\n",
    "        \n",
    "        all_rows.append(row)\n",
    "\n",
    "big_df = pd.DataFrame(data=all_rows, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Open'] + all_cats\n",
    "X = big_df[feature_cols]\n",
    "\n",
    "y_0d = big_df['r_0d']\n",
    "y_1d = big_df['r_1d']\n",
    "y_7d = big_df['r_7d']\n",
    "y_30d = big_df['r_30d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dcf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EST = 1000\n",
    "LAMBDA = 0.05\n",
    "MAX_DEPTH = 6\n",
    "EVAL_MET = \"mse\"\n",
    "\n",
    "model_0d = xgb.XGBRegressor(\n",
    "    n_estimators = N_EST,\n",
    "    learning_rate = LAMBDA,\n",
    "    max_depth= MAX_DEPTH,\n",
    "    eval_metric= EVAL_MET\n",
    ")\n",
    "\n",
    "model_1d = xgb.XGBRegressor(\n",
    "    n_estimators = N_EST,\n",
    "    learning_rate = LAMBDA,\n",
    "    max_depth= MAX_DEPTH,\n",
    "    eval_metric= EVAL_MET\n",
    ")\n",
    "\n",
    "model_7d = xgb.XGBRegressor(\n",
    "    n_estimators = N_EST,\n",
    "    learning_rate = LAMBDA,\n",
    "    max_depth= MAX_DEPTH,\n",
    "    eval_metric= EVAL_MET\n",
    ")\n",
    "\n",
    "model_30d = xgb.XGBRegressor(\n",
    "    n_estimators = N_EST,\n",
    "    learning_rate = LAMBDA,\n",
    "    max_depth= MAX_DEPTH,\n",
    "    eval_metric= EVAL_MET\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
