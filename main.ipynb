{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad32db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8c423",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "177b96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Local Dataset Importing\n",
    "        Originally tried to use the Kaggle API however quickly faced\n",
    "        issues with rate limiting. Local downloading will speed up\n",
    "        training at the cost of repeatability for others viewing project.\n",
    "        \n",
    "        Solution, I pull from downloaded datasets in parent folder to this\n",
    "        github repository. news_data contains only the news dataset .json file,\n",
    "        stock data contains a .csv with the metadata as well as 2 subfolders\n",
    "        ETF and Stocks that contain their respective ticker histories\n",
    "\"\"\"\n",
    "news_path = os.getcwd() + \"/../news_data/\"\n",
    "stock_path = os.getcwd() + \"/../stock_data/\"\n",
    "\n",
    "news_df = pd.read_json(f'{news_path}News_Category_Dataset_v3.json', lines=True)\n",
    "stock_meta = pd.read_csv(f'{stock_path}symbols_valid_meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d8e0d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with stock: AGM$A\n",
      "Error with stock: CARR.V\n",
      "Error with stock: UTX.V\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Capture all Data Frames\n",
    "        Maintain a dictionary with keys as stock ticker symbols and values\n",
    "        as the DataFrames captured from reading the stock's .csv\n",
    "\"\"\"\n",
    "stock_tickers = [x['Symbol'] for _, x in stock_meta.iterrows() if x['ETF'] == 'N'] # Remove ETFs\n",
    "\n",
    "\n",
    "stock_dfs = {}\n",
    "for s in stock_tickers:\n",
    "    try:\n",
    "        stock_dfs[s] = pd.read_csv(f\"{stock_path}/stocks/{s}.csv\")\n",
    "    except Exception:\n",
    "        print(f\"Error with stock: {s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f7606494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Columns: ['headline' 'category' 'date' 'effective_date']\n",
      "Stock Columns: ['Date' 'Open' 'High' 'Low' 'Close' 'Adj Close' 'Volume']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset Columns\n",
    "\"\"\"\n",
    "print(f\"News Columns: {news_df.columns.values}\")\n",
    "print(f\"Stock Columns: {stock_dfs['A'].columns.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0f1058e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "# Return the next trading day, avoiding holidays and weekends\n",
    "#   Inputs\n",
    "#       cur_day      -> The current date\n",
    "#       trading_days -> List of all open dates in the range\n",
    "#   Returns:\n",
    "#       The next available date (datetime object)\n",
    "def next_trading_day(cur_day, trading_days):\n",
    "    days_left = trading_days[trading_days > cur_day]\n",
    "    return days_left.min() if len(days_left) else trading_days.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Prepare Stock Datasets\n",
    "        Drop irrelevant columns --> High, Low, Volume, adj_close\n",
    "        Drop information prior to 2012-01-28\n",
    "        Create stock return time horizon features\n",
    "\"\"\"\n",
    "processed_stocks = {}\n",
    "tickers = stock_dfs.keys()\n",
    "\n",
    "for s in tickers:\n",
    "    df = stock_dfs[s].copy()\n",
    "\n",
    "    df['Ticker'] = s\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df[df['Date'] >= pd.Timestamp(\"2012-01-28\")].reset_index(drop=True)\n",
    "    df = df.drop(['High', 'Low', 'Adj Close', 'Volume'], axis=1)\n",
    "\n",
    "    if df.shape[0] != 2057: # 2057 dates 2012-01-28 and 2020-04-01, cut unfilled stocks\n",
    "        continue\n",
    "\n",
    "    df['r_0d'] = (df['Close'] - df['Open'])/df['Open']\n",
    "    df['r_1d'] = df['Close'].shift(-1).pct_change(periods=1, fill_method=None)\n",
    "    df['r_7d'] = df['Close'].shift(-7).pct_change(periods=7, fill_method=None)\n",
    "    df['r_30d'] = df['Close'].shift(-30).pct_change(periods=30, fill_method=None)\n",
    "\n",
    "    processed_stocks[s] = df\n",
    "\n",
    "print(f\"Number of Stocks before processing: \\t{len(stock_dfs)}\")\n",
    "print(f\"Number of Stocks after processing: \\t{len(processed_stocks)}\")\n",
    "\n",
    "stock_dfs = {k: v for k, v in processed_stocks.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "82939532",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'A'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstock_dfs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mA\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mKeyError\u001b[39m: 'A'"
     ]
    }
   ],
   "source": [
    "print(stock_dfs['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefdb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Prepare News Dataset\n",
    "        News DF starts 2022-09-23, ends 2012-01-28 --> First must reverse dataset\n",
    "        Drop categories to only necessary --> Category, Headline, Date\n",
    "        Shift dates to align with trading days, skipping weekends and holidays till next open day\n",
    "\"\"\"\n",
    "news_df = news_df.sort_values(by='date', ascending=True).reset_index(drop=True)\n",
    "news_df = news_df.drop(['link', 'short_description', 'authors'], axis=1)\n",
    "news_df = news_df[news_df['date'] <= pd.Timestamp(\"2020-04-01\")].reset_index(drop=True)\n",
    "\n",
    "trading_days = pd.to_datetime(stock_dfs['A']['Date'].unique())\n",
    "news_df['effective_date'] = news_df['date'].apply(lambda d: next_trading_day(d, trading_days))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
